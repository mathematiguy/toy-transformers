{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCN4h6JTQOD0"
   },
   "source": [
    "# Try SpanBERT, then work on multitoken\n",
    "\n",
    "## trying SpanBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b4kbRA5X5TG8"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from functional import pseq, seq\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "id": "e1wDCo-3Rerd",
    "outputId": "439a75cb-d4c2-4017-accf-afbf7c52fe0b"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yNoL-dH9RmEp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d8982267a741b7ba2e46ad8dabf9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12583abb1ef24713b12caba63f5d7fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertForMaskedLM.from_pretrained('bert-base-uncased') # have to pass the directory!!! ARG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NH08QN55RpLH",
    "outputId": "9dbad4ac-383f-48b6-bbc6-d838de83d038"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5b5515618f40f6b09b340994bd8e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_path = 'bert-base-cased' # uses same tokenizer in bert/spanbert\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(tok_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NWfbWhDOVHu1",
    "outputId": "e9fdc4f4-2bd3-4b4e-92ad-ad3ef919898e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "HWQNpNWgVkSv",
    "outputId": "4db42cab-7472-44ee-ab44-c6c00cdeccfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "using custom model: ['BertForMaskedLM']\n"
     ]
    }
   ],
   "source": [
    "from fitbert import FitBert\n",
    "\n",
    "fb = FitBert(model=bert, tokenizer=tokenizer, disable_gpu=True)\n",
    "# Note:\n",
    "# I'm SURE I had a reason to disable_gpu... but I wish I'd left a fucking note as to why it was..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "X5wO7sA0WrHP",
    "outputId": "5c18836c-0f2c-4107-ca32-770db7aa478e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# please be true...\n",
    "fb.bert == bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "E3u-bRfKYGQW",
    "outputId": "fae8d0ff-9b6b-4e0b-9f8d-3ff4b61f7640"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dZQAFgJ3W8AH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to in', 'out in', 'out of the closet in']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the way FB currently works, this just looks at the first token\n",
    "\n",
    "fb.rank(\"the first Star Wars came ***mask*** 1977\", [\"out in\", \"to in\", \"out of the closet in\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGPTjFHTk68H"
   },
   "source": [
    "### Rewrite rank_multi to use tensors\n",
    "\n",
    "#### THIS IS THE MAIN FOCUS OF THIS NOTEBOOK... it happens to use spanbert, but I don't think that is important\n",
    "\n",
    "However this implementation uses loops, and should be all tensor ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tN2Nbb3zXbCa"
   },
   "outputs": [],
   "source": [
    "# looking at the code, rank_multi is not gonna work\n",
    "# so rewrite?\n",
    "\n",
    "def new_rank_multi(self, masked_sent: str, words: List[str]):\n",
    "\n",
    "    words_ids = [ self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(lst)) for lst in words ]\n",
    "\n",
    "    print(\"word ids: \", words_ids)\n",
    "\n",
    "    lens = [ len(x) for x in words_ids ]\n",
    "\n",
    "    print(\"lengths of each list in word ids: \", lens)\n",
    "\n",
    "    final_ranked_options = []\n",
    "    final_ranked_options_prob = []\n",
    "\n",
    "    pre, post = masked_sent.split(self.mask_token)\n",
    "\n",
    "    if post[-1] not in [\".\", \",\", \"?\", \"!\", \";\", \":\"]:\n",
    "        post += \".\"\n",
    "\n",
    "    if all([x == 1 for x in lens]):\n",
    "        # this is just rank_single for inspiration\n",
    "        tokens = [\"[CLS]\"] + self.tokenizer.tokenize(pre)\n",
    "        target_idx = len(tokens)\n",
    "        tokens += [\"[MASK]\"]\n",
    "        tokens += self.tokenizer.tokenize(post)\n",
    "\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tens = torch.tensor(input_ids).unsqueeze(0)\n",
    "        tens = tens.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            preds = self.bert(tens)[0]\n",
    "            probs = self.softmax(preds)\n",
    "\n",
    "            ranked_pairs = (\n",
    "                seq(words_ids)\n",
    "                .map(lambda x: float(probs[0][target_idx][x].item()))\n",
    "                .zip(words)\n",
    "                .sorted(key=lambda x: x[0], reverse=True)\n",
    "            )\n",
    "\n",
    "            ranked_options = (seq(ranked_pairs).map(lambda x: x[1])).list()\n",
    "            ranked_options_prob = (seq(ranked_pairs).map(lambda x: x[0])).list()\n",
    "\n",
    "            del tens, preds, probs, tokens, words_ids, input_ids\n",
    "            if self.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            return ranked_options, ranked_options_prob\n",
    "    else:\n",
    "        for words_idx, mask_len in enumerate(lens):\n",
    "            # FUCK\n",
    "            # this shouldn't be a loop, it should be one big tensor [len(word_ids), num_masked_tokens, vocab_size]\n",
    "            # might need to pad so when num_masked_tokens is less than the longest mask, they all end up the same shape\n",
    "            #\n",
    "            # actually, it should be even bigger, because it should be batched,\n",
    "            # [batch_size, len(word_ids), num_masked_tokens, vocab_size]\n",
    "            print(f\"mask len = {mask_len}\")\n",
    "            \n",
    "            tokens = [\"[CLS]\"] + self.tokenizer.tokenize(pre)\n",
    "            target_idx_start = len(tokens)\n",
    "            target_idx_end = target_idx_start + mask_len\n",
    "            tokens += [\"[MASK]\"] * mask_len\n",
    "            tokens += self.tokenizer.tokenize(post)  # no [SEP] b/c SpanBERT doesn't use\n",
    "            print(\"there are this many tokens \", len(tokens))\n",
    "            print(\"they are \", tokens)\n",
    "            \n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            tens = torch.tensor(input_ids).unsqueeze(0)\n",
    "            tens = tens.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                # @todo don't [0], that assumes batch_size == 1\n",
    "                preds = self.bert(tens)[0]\n",
    "                # @TODO don't softmax yet maybe... ok it seems to work. Maybe softmax again at the end?\n",
    "                probs = self.softmax(preds)\n",
    "\n",
    "                # @TODO when this is all one batch instead of a loop, this will have to be matrix multiplication\n",
    "                # start and end will be different depending on the mask length\n",
    "                # so need to construct a sparse matrix to use to multiply out the values desired (eg the indecise that were masked)\n",
    "                masked_probs = probs[0][target_idx_start : target_idx_end]\n",
    "\n",
    "                # masked_probs has size [num_masked_tokens, vocab_size]\n",
    "\n",
    "                print(f\"the masked probs are {masked_probs} \\n and its shape is {masked_probs.shape}\")\n",
    "\n",
    "                # want to pick out the probs corresponding to the word ids\n",
    "\n",
    "                assert masked_probs.shape[0] == mask_len, \"there is a row for each word id\"\n",
    "\n",
    "                a = torch.zeros_like(masked_probs)\n",
    "                \n",
    "                for i, word_id in enumerate(words_ids[words_idx]):\n",
    "                    a[i][word_id] = 1\n",
    "\n",
    "                a = torch.transpose(a, 0, 1)\n",
    "\n",
    "                print(\"a's shape is \", a.shape)\n",
    "\n",
    "                mm = torch.matmul(masked_probs, a)\n",
    "\n",
    "                print(\"mm result: \", mm)\n",
    "\n",
    "                # only care about the diagonal values on mm (this was confusing, but I think is right)\n",
    "                word_probs = torch.diag(mm)\n",
    "                # why product? because a long span can have one very likely word, which throws off max and avg too much\n",
    "                span_prob = torch.prod(word_probs).item()\n",
    "\n",
    "                print(\"span probs: \", span_prob, \"... words: \", words[words_idx])\n",
    "\n",
    "                final_ranked_options.append(words[words_idx])\n",
    "                final_ranked_options_prob.append(span_prob)\n",
    "        print(sorted(zip(final_ranked_options_prob, final_ranked_options), reverse=True))\n",
    "        final_ranked_options_prob, final_ranked_options = zip(*sorted(zip(final_ranked_options_prob, final_ranked_options), reverse=True))\n",
    "        return final_ranked_options, final_ranked_options_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "id": "zt-kaAehaY7J",
    "outputId": "801c6928-5485-4ae3-c548-6f8f600359cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ids:  [[1149, 1107], [1106, 1107], [1121, 12477, 1733, 1106, 4033, 1107]]\n",
      "lengths of each list in word ids:  [2, 2, 6]\n",
      "mask len = 2\n",
      "there are this many tokens  10\n",
      "they are  ['[CLS]', 'the', 'first', 'Star', 'Wars', 'came', '[MASK]', '[MASK]', '1977', '.']\n",
      "the masked probs are tensor([[3.0007e-07, 3.0925e-07, 2.9548e-07,  ..., 6.1910e-07, 7.8386e-07,\n",
      "         4.3798e-06],\n",
      "        [2.8629e-07, 2.8874e-07, 2.7992e-07,  ..., 5.0310e-07, 8.5147e-07,\n",
      "         3.7964e-06]]) \n",
      " and its shape is torch.Size([2, 30522])\n",
      "a's shape is  torch.Size([30522, 2])\n",
      "mm result:  tensor([[1.2060e-05, 3.8723e-06],\n",
      "        [1.2404e-05, 2.1813e-06]])\n",
      "span probs:  2.630639976686222e-11 ... words:  out in\n",
      "mask len = 2\n",
      "there are this many tokens  10\n",
      "they are  ['[CLS]', 'the', 'first', 'Star', 'Wars', 'came', '[MASK]', '[MASK]', '1977', '.']\n",
      "the masked probs are tensor([[3.0007e-07, 3.0925e-07, 2.9548e-07,  ..., 6.1910e-07, 7.8386e-07,\n",
      "         4.3798e-06],\n",
      "        [2.8629e-07, 2.8874e-07, 2.7992e-07,  ..., 5.0310e-07, 8.5147e-07,\n",
      "         3.7964e-06]]) \n",
      " and its shape is torch.Size([2, 30522])\n",
      "a's shape is  torch.Size([30522, 2])\n",
      "mm result:  tensor([[9.9483e-06, 3.8723e-06],\n",
      "        [9.3768e-06, 2.1813e-06]])\n",
      "span probs:  2.1699878005598805e-11 ... words:  to in\n",
      "mask len = 6\n",
      "there are this many tokens  14\n",
      "they are  ['[CLS]', 'the', 'first', 'Star', 'Wars', 'came', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '1977', '.']\n",
      "the masked probs are tensor([[4.3394e-07, 4.4632e-07, 4.2795e-07,  ..., 7.1578e-07, 9.0684e-07,\n",
      "         4.5055e-06],\n",
      "        [4.2032e-07, 4.2384e-07, 4.1089e-07,  ..., 6.5639e-07, 9.1908e-07,\n",
      "         4.0710e-06],\n",
      "        [3.9057e-07, 3.9059e-07, 3.8004e-07,  ..., 5.8601e-07, 9.1482e-07,\n",
      "         3.5881e-06],\n",
      "        [3.9351e-07, 3.9279e-07, 3.8375e-07,  ..., 5.6564e-07, 9.4520e-07,\n",
      "         3.0669e-06],\n",
      "        [4.0948e-07, 4.0741e-07, 3.9992e-07,  ..., 5.5158e-07, 9.6934e-07,\n",
      "         2.6522e-06],\n",
      "        [4.0944e-07, 4.0477e-07, 3.9950e-07,  ..., 5.0956e-07, 9.6119e-07,\n",
      "         2.2686e-06]]) \n",
      " and its shape is torch.Size([6, 30522])\n",
      "a's shape is  torch.Size([30522, 6])\n",
      "mm result:  tensor([[1.5417e-06, 9.6647e-06, 4.3467e-06, 6.6866e-06, 2.0193e-05, 2.9909e-06],\n",
      "        [1.4523e-06, 9.3292e-06, 3.6309e-06, 6.0871e-06, 1.7644e-05, 2.3599e-06],\n",
      "        [1.2558e-06, 9.1292e-06, 2.8885e-06, 5.3654e-06, 1.6749e-05, 1.8580e-06],\n",
      "        [1.1166e-06, 1.0125e-05, 2.5154e-06, 5.1157e-06, 1.5430e-05, 1.7146e-06],\n",
      "        [1.0795e-06, 1.1412e-05, 2.3963e-06, 5.2026e-06, 1.4372e-05, 1.7463e-06],\n",
      "        [1.0230e-06, 1.4462e-05, 1.8185e-06, 5.0401e-06, 1.3678e-05, 1.4279e-06]])\n",
      "span probs:  4.361758114092486e-33 ... words:  from mars to earth in\n",
      "[(2.630639976686222e-11, 'out in'), (2.1699878005598805e-11, 'to in'), (4.361758114092486e-33, 'from mars to earth in')]\n"
     ]
    }
   ],
   "source": [
    "mask_opts, mask_probs = new_rank_multi(fb, \"the first Star Wars came ***mask*** 1977\", [\"out in\", \"to in\", \"from mars to earth in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "MJtYzDgyamUD",
    "outputId": "1c163929-4a83-42ba-a712-791222f6c598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.630639976686222e-11, 2.1699878005598805e-11, 4.361758114092486e-33)\n",
      "[1.0, 0.8248896921628104, 1.6580596937430137e-22]\n",
      "tensor([[0.4531, 0.3803, 0.1667]])\n"
     ]
    }
   ],
   "source": [
    "# Not sure which is the best format for these... I think scores?\n",
    "\n",
    "print(mask_probs)\n",
    "scores = [x / max(mask_probs) for x in mask_probs ]\n",
    "print(scores)\n",
    "\n",
    "print(fb.softmax(torch.tensor(scores).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vpCr7j1Y68jq",
    "outputId": "b8670fa9-c0fc-4fb6-ab87-16bdbadf2462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('out in', 'to in', 'from mars to earth in')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EAhdH0-j7FNi"
   },
   "outputs": [],
   "source": [
    "rm = lambda x, y: new_rank_multi(fb, x, y)\n",
    "fb.rank_multi = rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "PQJLms2x9wQ3",
    "outputId": "36546f68-eee9-4f09-9a06-bbc47bf8e657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ids:  [[1149, 1107], [1106, 1121], [1149, 1104, 1103, 9369, 1107]]\n",
      "lengths of each list in word ids:  [2, 2, 5]\n",
      "mask len = 2\n",
      "there are this many tokens  10\n",
      "they are  ['[CLS]', 'The', 'first', 'Star', 'Wars', 'came', '[MASK]', '[MASK]', '1977', '.']\n",
      "the masked probs are tensor([[3.8999e-07, 3.9552e-07, 3.7480e-07,  ..., 5.6233e-07, 8.2060e-07,\n",
      "         5.2370e-06],\n",
      "        [3.9211e-07, 3.8405e-07, 3.7379e-07,  ..., 5.3387e-07, 8.4046e-07,\n",
      "         4.8882e-06]]) \n",
      " and its shape is torch.Size([2, 30522])\n",
      "a's shape is  torch.Size([30522, 2])\n",
      "mm result:  tensor([[6.0911e-06, 3.5360e-06],\n",
      "        [6.2990e-06, 2.8287e-06]])\n",
      "span probs:  1.7230000412538082e-11 ... words:  out in\n",
      "mask len = 2\n",
      "there are this many tokens  10\n",
      "they are  ['[CLS]', 'The', 'first', 'Star', 'Wars', 'came', '[MASK]', '[MASK]', '1977', '.']\n",
      "the masked probs are tensor([[3.8999e-07, 3.9552e-07, 3.7480e-07,  ..., 5.6233e-07, 8.2060e-07,\n",
      "         5.2370e-06],\n",
      "        [3.9211e-07, 3.8405e-07, 3.7379e-07,  ..., 5.3387e-07, 8.4046e-07,\n",
      "         4.8882e-06]]) \n",
      " and its shape is torch.Size([2, 30522])\n",
      "a's shape is  torch.Size([30522, 2])\n",
      "mm result:  tensor([[7.2966e-06, 1.4038e-06],\n",
      "        [7.3815e-06, 1.2931e-06]])\n",
      "span probs:  9.434898175231243e-12 ... words:  to from\n",
      "mask len = 5\n",
      "there are this many tokens  13\n",
      "they are  ['[CLS]', 'The', 'first', 'Star', 'Wars', 'came', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '1977', '.']\n",
      "the masked probs are tensor([[3.9162e-07, 4.0565e-07, 3.8216e-07,  ..., 5.8913e-07, 7.8436e-07,\n",
      "         5.3674e-06],\n",
      "        [3.7655e-07, 3.8027e-07, 3.6540e-07,  ..., 5.5221e-07, 7.8249e-07,\n",
      "         4.6807e-06],\n",
      "        [3.5983e-07, 3.6048e-07, 3.4887e-07,  ..., 5.2241e-07, 7.7649e-07,\n",
      "         4.1820e-06],\n",
      "        [3.5985e-07, 3.5977e-07, 3.4963e-07,  ..., 5.1273e-07, 7.7407e-07,\n",
      "         3.8693e-06],\n",
      "        [3.6728e-07, 3.6260e-07, 3.5536e-07,  ..., 4.8482e-07, 7.7104e-07,\n",
      "         3.6794e-06]]) \n",
      " and its shape is torch.Size([5, 30522])\n",
      "a's shape is  torch.Size([30522, 5])\n",
      "mm result:  tensor([[6.6720e-06, 2.8843e-06, 3.9257e-06, 2.3881e-05, 2.3846e-06],\n",
      "        [6.2856e-06, 2.5838e-06, 3.8951e-06, 2.1984e-05, 1.7957e-06],\n",
      "        [5.8784e-06, 2.4794e-06, 3.9756e-06, 2.0082e-05, 1.5871e-06],\n",
      "        [5.2781e-06, 2.4312e-06, 4.1141e-06, 1.8827e-05, 1.5534e-06],\n",
      "        [4.0308e-06, 2.2805e-06, 3.4797e-06, 1.6223e-05, 1.2671e-06]])\n",
      "span probs:  1.6349644928420955e-27 ... words:  out of the closet in\n",
      "[(1.7230000412538082e-11, 'out in'), (9.434898175231243e-12, 'to from'), (1.6349644928420955e-27, 'out of the closet in')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The first Star Wars came out in 1977'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb.fitb(\"The first Star Wars came ***mask*** 1977\", [\"to from\", \"out in\", \"out of the closet in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HOWdqZvr_4Ha",
    "outputId": "320b1c48-414b-47c4-b95f-a5b0f6e535c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from', 'ma', '##rs', 'to', 'earth', 'in']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fb.tokenizer.ids_to_tokens[wid] for wid in [1121, 12477, 1733, 1106, 4033, 1107]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaSLuUKbALwh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Try SpanBERT and multitoken masking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
