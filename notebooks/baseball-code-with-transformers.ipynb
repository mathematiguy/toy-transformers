{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseball code with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with transformers. We wanna use masking to generate ciphertexts for simple messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc47d28480d416cb97a63812e67762e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3df9a0c8177435488b42bbbdc15024b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f13365c7264227acbeb21563670807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30e342f5c67493da3454a2e1d93dacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../plaintext.txt',  'r') as f:\n",
    "    plaintext = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're no strangers to love\n",
      "You know the rules and so do I\n",
      "A full commitment's what I'm thinking of\n",
      "You wouldn't get this from any other guy\n",
      "I just wanna tell you how I'm feeling\n",
      "Gotta make you understand\n",
      "Never gonna give you up\n",
      "Never gonna let you down\n",
      "Never gonna run around and desert you\n",
      "Never gonna make you cry\n",
      "Never gonna say goodbye\n",
      "Never gonna tell a lie and hurt you\n",
      "We've known each other for so long\n"
     ]
    }
   ],
   "source": [
    "# Here is the plaintext:\n",
    "print(plaintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_length = 3\n",
    "\n",
    "masked = []\n",
    "for line in plaintext.split('\\n'):\n",
    "    words = line.split(' ')\n",
    "    masked_words = []\n",
    "    while len(words) > 0:\n",
    "        if len(masked_words) % code_length == 0:\n",
    "            masked_words.append(words.pop(0))\n",
    "        else:\n",
    "            masked_words.append('[MASK]')\n",
    "    masked.append(' '.join(masked_words))\n",
    "masked = '\\n'.join(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're [MASK] [MASK] no [MASK] [MASK] strangers [MASK] [MASK] to [MASK] [MASK] love\n",
      "You [MASK] [MASK] know [MASK] [MASK] the [MASK] [MASK] rules [MASK] [MASK] and [MASK] [MASK] so [MASK] [MASK] do [MASK] [MASK] I\n",
      "A [MASK] [MASK] full [MASK] [MASK] commitment's [MASK] [MASK] what [MASK] [MASK] I'm [MASK] [MASK] thinking [MASK] [MASK] of\n",
      "You [MASK] [MASK] wouldn't [MASK] [MASK] get [MASK] [MASK] this [MASK] [MASK] from [MASK] [MASK] any [MASK] [MASK] other [MASK] [MASK] guy\n",
      "I [MASK] [MASK] just [MASK] [MASK] wanna [MASK] [MASK] tell [MASK] [MASK] you [MASK] [MASK] how [MASK] [MASK] I'm [MASK] [MASK] feeling\n",
      "Gotta [MASK] [MASK] make [MASK] [MASK] you [MASK] [MASK] understand\n",
      "Never [MASK] [MASK] gonna [MASK] [MASK] give [MASK] [MASK] you [MASK] [MASK] up\n",
      "Never [MASK] [MASK] gonna [MASK] [MASK] let [MASK] [MASK] you [MASK] [MASK] down\n",
      "Never [MASK] [MASK] gonna [MASK] [MASK] run [MASK] [MASK] around [MASK] [MASK] and [MASK] [MASK] desert [MASK] [MASK] you\n",
      "Never [MASK] [MASK] gonna [MASK] [MASK] make [MASK] [MASK] you [MASK] [MASK] cry\n",
      "Never [MASK] [MASK] gonna [MASK] [MASK] say [MASK] [MASK] goodbye\n",
      "Never [MASK] [MASK] gonna [MASK] [MASK] tell [MASK] [MASK] a [MASK] [MASK] lie [MASK] [MASK] and [MASK] [MASK] hurt [MASK] [MASK] you\n",
      "We've [MASK] [MASK] known [MASK] [MASK] each [MASK] [MASK] other [MASK] [MASK] for [MASK] [MASK] so [MASK] [MASK] long\n"
     ]
    }
   ],
   "source": [
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineException",
     "evalue": "More than one mask_token ([MASK]) is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-dc6de8cdfc01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, targets, top_k, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m                 \u001b[0;31m# Fill mask pipeline supports only one ${mask_token} per sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_exactly_one_mask_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36mensure_exactly_one_mask_token\u001b[0;34m(self, masked_index)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                 \u001b[0;34m\"fill-mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                 \u001b[0;34mf\"More than one mask_token ({self.tokenizer.mask_token}) is not supported\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             )\n\u001b[1;32m   1209\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPipelineException\u001b[0m: More than one mask_token ([MASK]) is not supported"
     ]
    }
   ],
   "source": [
    "unmasked = unmasker(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
